{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/AmItheAsshole Lanugage Analysis\n",
    "\n",
    "### Introduction\n",
    "\n",
    "[r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/) is a subreddit where people will post a story about some conflict they've had and it is up to the community to judge who the \"asshole\" is in the situation. The page description words this very eloquently:\n",
    "\n",
    "\"A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that's been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you're right, or you're the asshole.\" ([r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/))\n",
    "\n",
    "The goal of this project if there are any notable lexical features between the langauge of the \"assholes\" and those who are not.\n",
    "\n",
    "One thing that I would like to note is that this is not a corpus directly translated from speech so the language of the mosts might have been thoughfully worded and might not be representative of ones's natural speech. Still I believe that the judgments voted on by the reddit community can provide insight on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping from Reddit\n",
    "#### PRAW\n",
    "[The Python Reddit API Wrapper (PRAW)](https://praw.readthedocs.io/en/v4.1.0/index.html) is an API for anything you can do in Reddit. In this project it is used to scrape posts from the r/AmItheAsshole subreddit.\n",
    "\n",
    "#### The r/AmItheAsshole Archives\n",
    "The subreddit archives the posts into 4 different categories related to this project. There are 5 judgements that one can decide on, however the last one that is not archived 'INFO' (Not enough info) does not help with this investigation. The different judgements are described in the function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for scraping from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import praw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "\n",
    "# Initialize reddit API\n",
    "reddit = praw.Reddit('scraper', user_agent='corpus ling project')\n",
    "reddit.read_only = True\n",
    "\n",
    "# Abbreviations for the archive queries\n",
    "archive_names = { 'TYA' : 'Asshole', 'NTA' : 'Not the A-hole', 'ESH' : 'Everyone Sucks',\n",
    "        'NAH' : 'No A-holes here'}\n",
    "\n",
    "# Pandas dataframe containing all of the posts.\n",
    "corpus = pd.DataFrame()\n",
    "\n",
    "def get_archive(judgment):\n",
    "    '''\n",
    "    Get the posts from a single archive of r/AmItheAsshole\n",
    "    Archive judgement options are:\n",
    "        'TYA' -- You're the Asshole (& the other party is not)\n",
    "        'NTA' -- You're Not the A-hole (& the other party is)\n",
    "        'ESH' -- Everyone Sucks Here\n",
    "        'NAH' -- No A-holes here\n",
    "    '''\n",
    "    return reddit.subreddit('AmItheAsshole').search(\n",
    "            'flair_name:\"' + archive_names[judgment] + '\"', limit=None)\n",
    "\n",
    "def load_archives(download_local = True):\n",
    "    '''\n",
    "    Grabs each of the archives and puts them in the corpus dictionary.\n",
    "    The download_local flag determines if you want to download the text\n",
    "    as well in the current directory.\n",
    "    '''\n",
    "    global corpus \n",
    "    corpus = pd.DataFrame()\n",
    "    for judgement in tqdm_notebook(archive_names.keys()):\n",
    "        posts = get_archive(judgement)\n",
    "        row_df = pd.io.json.json_normalize(\n",
    "                map(lambda post : {'archive' : judgement, **vars(post)}, posts))\n",
    "        # store only the id, title, archive, rawtext, and url\n",
    "        corpus = corpus.append(row_df[['id', 'title', 'archive', 'selftext', 'url']],\n",
    "                               ignore_index = True, sort=True)        \n",
    "        \n",
    "        # Download the text locally\n",
    "        if (download_local):\n",
    "            if not os.path.exists(judgement):\n",
    "                # separate archives into different directories.\n",
    "                os.mkdir(judgement)\n",
    "            for index, post in (corpus.loc[corpus['archive'] == judgement]).iterrows():\n",
    "                # Save each file in their respective directories with their id as the filename\n",
    "                with open(judgement + '/' + post.id + '.yaml', 'w') as file:\n",
    "                    documents = yaml.dump(post.to_dict(), file)\n",
    "\n",
    "def local_load_archives():\n",
    "    '''\n",
    "    Loads the archived posts from yaml files in the current directory.\n",
    "    '''\n",
    "    global corpus \n",
    "    corpus = pd.DataFrame()\n",
    "    for judgement in tqdm_notebook(archive_names.keys()):\n",
    "        for filename in glob.glob(judgement + '/*.yaml'):\n",
    "            with open(filename) as file:\n",
    "                corpus = corpus.append(pd.io.json.json_normalize(\n",
    "                    yaml.load(file, Loader=yaml.FullLoader)),\n",
    "                                       ignore_index = True, sort=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several options for loading the corpus of posts:\n",
    "To keep things consistent, please use the `local_load_archives()` option becuase posts might be added over time and so the anaysis of this corpus may change as new posts are added.\n",
    "\n",
    "Note: `load_archives()` will not work unless you have a `praw.ini` file in the current directory. This file contains the necesary account information for using the Reddit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you just want to use the posts that are downloaded locally.\n",
    "local_load_archives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want the most recent set of posts but without downloading them locally.\n",
    "load_archives(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want to load and update the local files with the current archives.\n",
    "load_archives()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The corpus\n",
    "The corpus of posts is represented as a datarame whos most important columns are the archive column, and the text. The following code prints out summaries of all of the archives with the counts of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for judgement in archive_names.keys():\n",
    "    print(\"\\n\\n Summary of all of the posts in the \\\"\" + archive_names[judgement] + \"\\\" archive:\")\n",
    "    print(corpus.loc[corpus['archive'] == judgement].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feel free to take a break and read a random post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = corpus.sample(n=1)\n",
    "print(post.selftext.tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Can you guess how the community judged this submission?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(archive_names[post.archive.tolist()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Corpus\n",
    "#### NLTK\n",
    "[Natural Language Toolkit (NLTK)](https://www.nltk.org/) is a python library with a lot of Natural Language processing tools.\n",
    "\n",
    "First we need to import nltk and download some tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions for using the nltk library on the corpus dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize, Text, FreqDist\n",
    "\n",
    "def corpus_tokenize():\n",
    "    '''\n",
    "    Converts the text of each post to a list of tokens and places\n",
    "    them in a list in a column within the corpus dataframe.\n",
    "    '''\n",
    "    global corpus\n",
    "    corpus['tokens'] = list(map(lambda text : word_tokenize(text.lower()), corpus['selftext']))\n",
    "    # Get rid of punctuation\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    corpus['tokens_no_punct'] = list(map(lambda text : tokenizer.tokenize(text.lower()), corpus['selftext']))\n",
    "    # Tokenize on sentences.\n",
    "    corpus['sent_tokens'] = list(map(sent_tokenize, corpus['selftext']))\n",
    "    \n",
    "def get_archive_texts(archives, no_punctuation = False):\n",
    "    '''\n",
    "    Gets the texts of every post of the archives listed in archives\n",
    "    and returns it as one list of words.\n",
    "    '''\n",
    "    global corpus\n",
    "    texts = []\n",
    "    for archive in archives:\n",
    "        for post_tokens in corpus.loc[corpus['archive'] == archive]['tokens_no_punct' if no_punctuation else 'tokens'].to_list():\n",
    "            texts += post_tokens\n",
    "    return texts\n",
    "\n",
    "def get_archive_texts_sentences(archives):\n",
    "    '''\n",
    "    Gets the texts of every post of the archives listed in archives\n",
    "    and returns it as one list of sentences.\n",
    "    '''\n",
    "    global corpus\n",
    "    texts = []\n",
    "    for archive in archives:\n",
    "        for sent_tokens in corpus.loc[corpus['archive'] == archive]['sent_tokens'].to_list():\n",
    "            texts += sent_tokens\n",
    "    return texts\n",
    "\n",
    "def get_freq_dist(archives, no_stopwords = False):\n",
    "    '''\n",
    "    Get a FreqDist for the given archive names There is also the option to remove stopwords.\n",
    "    '''\n",
    "    text = get_archive_texts(archives, no_punctuation=True)\n",
    "    if no_stopwords:\n",
    "        # filter out stopwords\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        text = list(filter(lambda word : not word in stopwords, text))\n",
    "    return FreqDist(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the Corpus\n",
    "Creates three new columns in the corpus dataframe, 'tokens', 'tokens_no_punct', and 'sent_tokens'. One with just the raw output of the nltk tokenize function, the next without punctiation, and finally the text separated by sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenize()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequencies\n",
    "Since these posts are written in the first person and talk about personal experiences, a lot of the most frequent words are pronouns, therefore the following frequency counts remove stopwords. This can be changed by replacing `True` with `False` in the no_stopwords field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_freq_dist(archive_names.keys(), no_stopwords=True).most_common(20), columns=['Word', 'Frequencey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common = 50\n",
    "\n",
    "pd.DataFrame(get_freq_dist(['TYA', 'ESH'], no_stopwords=True).most_common(n_most_common),\n",
    "             columns=['word', 'frequencey']).join(\n",
    "    pd.DataFrame(get_freq_dist(['NTA', 'NAH'], no_stopwords=True).most_common(n_most_common),\n",
    "                 columns=['word', 'frequencey']), lsuffix=\"_a_hole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams and Collocations\n",
    "Lets explore making bigrams and trigrams and finding collocations for this corpus.\n",
    "\n",
    "First lets look at the collocations generated from bigrams. There are several different scoring measures that can be used as is listed in one of the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Feel free to change these values.\n",
    "n_collocs = 20\n",
    "n_freq_filter = 5\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# Options: pmi, raw_freq, likelihood_ratio, chi_sq, dice, phi_sq, etc.\n",
    "scoring_measure = bigram_measures.pmi\n",
    "\n",
    "flatten = lambda l: (l[0][0], l[0][1], l[1])\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(get_archive_texts(['TYA', 'ESH'], no_punctuation=True))\n",
    "finder.apply_freq_filter(n_freq_filter)\n",
    "a_hole_colloc = pd.DataFrame(list(map(flatten, finder.score_ngrams(scoring_measure)[:n_collocs])),\n",
    "                             columns=['bigram_word_1', 'bigram_word_2', 'score'])\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(get_archive_texts(['NTA', 'NAH'], no_punctuation=True))\n",
    "finder.apply_freq_filter(n_freq_filter)\n",
    "colloc = pd.DataFrame(list(map(flatten, finder.score_ngrams(scoring_measure)[:n_collocs])),\n",
    "                      columns=['bigram_word_1', 'bigram_word_2', 'score'])\n",
    "\n",
    "a_hole_colloc.join(colloc, lsuffix=\"_a_hole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about trigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Feel free to change these values.\n",
    "n_collocs = 20\n",
    "n_freq_filter = 5\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "# Options: pmi, raw_freq, likelihood_ratio, chi_sq, jaccard, dice, phi_sq, etc.\n",
    "scoring_measure = trigram_measures.pmi\n",
    "\n",
    "flatten = lambda l: (l[0][0], l[0][1], l[0][2], l[1])\n",
    "\n",
    "finder = TrigramCollocationFinder.from_words(get_archive_texts(['TYA', 'ESH'], no_punctuation=True))\n",
    "finder.apply_freq_filter(n_freq_filter)\n",
    "a_hole_colloc = pd.DataFrame(list(map(flatten, finder.score_ngrams(scoring_measure)[:n_collocs])),\n",
    "                             columns=['trigram_word_1', 'trigram_word_2', 'trigram_word_3', 'score'])\n",
    "\n",
    "finder = TrigramCollocationFinder.from_words(get_archive_texts(['NTA', 'NAH'], no_punctuation=True))\n",
    "finder.apply_freq_filter(n_freq_filter)\n",
    "colloc = pd.DataFrame(list(map(flatten, finder.score_ngrams(scoring_measure)[:n_collocs])),\n",
    "                      columns=['trigram_word_1', 'trigram_word_2', 'trigram_word_3', 'score'])\n",
    "\n",
    "a_hole_colloc.join(colloc, lsuffix=\"_a_hole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis\n",
    "NLTK provides a sentiment analysis model already called [VADER](https://github.com/cjhutto/vaderSentiment):\n",
    "\n",
    "\"VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\"\n",
    "\n",
    "The following are some functions to help process the text with the sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def get_average_sentiment_scores(text):\n",
    "    '''\n",
    "    Takes in a list of sentences to get the averge sentiment\n",
    "    scores for each sentence using VADER.\n",
    "    '''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    averages = {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "    count = 0\n",
    "    for sentence in text:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        for key, val in ss.items():\n",
    "            averages[key] += val\n",
    "        count += 1\n",
    "    for key in averages.keys():\n",
    "        averages[key] /= count\n",
    "    return averages\n",
    "\n",
    "def corpus_sentiment_scores():\n",
    "    '''\n",
    "    Updates the corpus with the average sentiment scores of each post.\n",
    "    '''\n",
    "    global corpus\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    scores = list(map(get_average_sentiment_scores, corpus['sent_tokens']))\n",
    "    corpus = corpus[corpus.columns.difference(scores[0].keys())].join(\n",
    "        pd.DataFrame(list(map(lambda d : d.values(), scores)), columns=scores[0].keys()))\n",
    "    \n",
    "def average_archive_sentiment_scores(archives):\n",
    "    '''\n",
    "    Gets the average sentiment scores for each archive and returns it\n",
    "    as a dataframe.\n",
    "    '''\n",
    "    global corpus\n",
    "    sentiment_scores = corpus.loc[\n",
    "        list(map(lambda row: row[1]['archive'] in archives,\n",
    "                 list(corpus.iterrows()))),\n",
    "        averages.keys()]\n",
    "    return sentiment_scores.mean(axis=0).to_dict()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first put these scores into the corpus dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "corpus_sentiment_scores()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do sentiment scores between the different judgemnts compare on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = pd.concat(\n",
    "    [pd.DataFrame(average_archive_sentiment_scores([archive]), index=[0]) for archive in archive_names.keys()],\n",
    "    ignore_index=True)\n",
    "\n",
    "archive_abr = list(archive_names.keys())\n",
    "sentiment_scores.rename(index=lambda i : archive_names[archive_abr[i]], inplace=True)\n",
    "sentiment_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
