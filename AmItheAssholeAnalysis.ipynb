{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/AmItheAsshole Lanugage Analysis\n",
    "\n",
    "### Introduction\n",
    "\n",
    "[r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/) is a subreddit where people will post a story about some conflict they've had and it is up to the community to judge who the \"asshole\" is in the situation. The page description words this very eloquently:\n",
    "\n",
    "\"A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that's been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you're right, or you're the asshole.\" ([r/AmItheAsshole](https://www.reddit.com/r/AmItheAsshole/))\n",
    "\n",
    "The goal of this project if there are any notable lexical features between the langauge of the \"assholes\" and those who are not.\n",
    "\n",
    "One thing that I would like to note is that this is not a corpus directly translated from speech so the language of the mosts might have been thoughfully worded and might not be representative of ones's natural speech. Still I believe that the judgments voted on by the reddit community can provide insight on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping from Reddit\n",
    "#### PRAW\n",
    "[The Python Reddit API Wrapper (PRAW)](https://praw.readthedocs.io/en/v4.1.0/index.html) is an API for anything you can do in Reddit. In this project it is used to scrape posts from the r/AmItheAsshole subreddit.\n",
    "\n",
    "#### The r/AmItheAsshole Archives\n",
    "The subreddit archives the posts into 4 different categories related to this project. There are 5 judgements that one can decide on, however the last one that is not archived 'INFO' (Not enough info) does not help with this investigation. The different judgements are described in the function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for scraping from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import praw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "\n",
    "# Initialize reddit API\n",
    "reddit = praw.Reddit('scraper', user_agent='corpus ling project')\n",
    "reddit.read_only = True\n",
    "\n",
    "# Abbreviations for the archive queries\n",
    "archive_names = { 'TYA' : 'Asshole', 'NTA' : 'Not the A-hole', 'ESH' : 'Everyone Sucks',\n",
    "        'NAH' : 'No A-holes here'}\n",
    "\n",
    "# Pandas dataframe containing all of the posts.\n",
    "corpus = pd.DataFrame()\n",
    "\n",
    "def get_archive(judgment):\n",
    "    '''\n",
    "    Get the posts from a single archive of r/AmItheAsshole\n",
    "    Archive judgement options are:\n",
    "        'TYA' -- You're the Asshole (& the other party is not)\n",
    "        'NTA' -- You're Not the A-hole (& the other party is)\n",
    "        'ESH' -- Everyone Sucks Here\n",
    "        'NAH' -- No A-holes here\n",
    "    '''\n",
    "    return reddit.subreddit('AmItheAsshole').search(\n",
    "            'flair_name:\"' + archive_names[judgment] + '\"', limit=None)\n",
    "\n",
    "def load_archives(download_local = True):\n",
    "    '''\n",
    "    Grabs each of the archives and puts them in the corpus dictionary.\n",
    "    The download_local flag determines if you want to download the text\n",
    "    as well in the current directory.\n",
    "    '''\n",
    "    global corpus \n",
    "    corpus = pd.DataFrame()\n",
    "    for judgement in tqdm_notebook(archive_names.keys()):\n",
    "        posts = get_archive(judgement)\n",
    "        row_df = pd.io.json.json_normalize(\n",
    "                map(lambda post : {'archive' : judgement, **vars(post)}, posts))\n",
    "        # store only the id, title, archive, rawtext, and url\n",
    "        corpus = corpus.append(row_df[['id', 'title', 'archive', 'selftext', 'url']],\n",
    "                               ignore_index = True, sort=True)        \n",
    "        \n",
    "        # Download the text locally\n",
    "        if (download_local):\n",
    "            if not os.path.exists(judgement):\n",
    "                # separate archives into different directories.\n",
    "                os.mkdir(judgement)\n",
    "            for index, post in (corpus.loc[corpus['archive'] == judgement]).iterrows():\n",
    "                # Save each file in their respective directories with their id as the filename\n",
    "                with open(judgement + '/' + post.id + '.yaml', 'w') as file:\n",
    "                    documents = yaml.dump(post.to_dict(), file)\n",
    "\n",
    "def local_load_archives():\n",
    "    '''\n",
    "    Loads the archived posts from yaml files in the current directory.\n",
    "    '''\n",
    "    global corpus \n",
    "    corpus = pd.DataFrame()\n",
    "    for judgement in tqdm_notebook(archive_names.keys()):\n",
    "        for filename in glob.glob(judgement + '/*.yaml'):\n",
    "            with open(filename) as file:\n",
    "                corpus = corpus.append(pd.io.json.json_normalize(\n",
    "                    yaml.load(file, Loader=yaml.FullLoader)),\n",
    "                                       ignore_index = True, sort=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several options for loading the corpus of posts:\n",
    "To keep things consistent, please use the `local_load_archives()` option becuase posts might be added over time and so the anaysis of this corpus may change as new posts are added.\n",
    "\n",
    "Note: `load_archives()` will not work unless you have a `praw.ini` file in the current directory. This file contains the necesary account information for using the Reddit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027c34e9fa0a4e51b1e9128b1b187d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this if you just want to use the posts that are downloaded locally.\n",
    "local_load_archives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375055a5c74f4439bb41076d059443c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this if you want the most recent set of posts but without downloading them locally.\n",
    "load_archives(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff72ead976804158b8119edccc5fb2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this if you want to load and update the local files with the current archives.\n",
    "load_archives()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The corpus\n",
    "The corpus of posts is represented as a datarame whos most important columns are the archive column, and the text. The following code prints out summaries of all of the archives with the counts of posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Summary of all of the posts in the \"Asshole\" archive:\n",
      "       archive      id                                           selftext  \\\n",
      "count      328     328                                                328   \n",
      "unique       1     328                                                328   \n",
      "top        TYA  eblrzx  Hi reddit. My husband and I were eating at chi...   \n",
      "freq       328       1                                                  1   \n",
      "\n",
      "                                                    title  \\\n",
      "count                                                 328   \n",
      "unique                                                328   \n",
      "top     AITA for riding my bike to work even though I ...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                      url  \n",
      "count                                                 328  \n",
      "unique                                                328  \n",
      "top     https://www.reddit.com/r/AmItheAsshole/comment...  \n",
      "freq                                                    1  \n",
      "\n",
      "\n",
      " Summary of all of the posts in the \"Not the A-hole\" archive:\n",
      "       archive      id                                           selftext  \\\n",
      "count      243     243                                                243   \n",
      "unique       1     243                                                243   \n",
      "top        NTA  ea4hnu  My mom remarried my stepdad 2 years ago. My st...   \n",
      "freq       243       1                                                  1   \n",
      "\n",
      "                                                    title  \\\n",
      "count                                                 243   \n",
      "unique                                                243   \n",
      "top     AITA for prioritizing my dog over my family Th...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                      url  \n",
      "count                                                 243  \n",
      "unique                                                243  \n",
      "top     https://www.reddit.com/r/AmItheAsshole/comment...  \n",
      "freq                                                    1  \n",
      "\n",
      "\n",
      " Summary of all of the posts in the \"Everyone Sucks\" archive:\n",
      "       archive      id                                           selftext  \\\n",
      "count      240     240                                                240   \n",
      "unique       1     240                                                240   \n",
      "top        ESH  e7ltup  Throwaway since he has an account. Iâ€™ve known ...   \n",
      "freq       240       1                                                  1   \n",
      "\n",
      "                              title  \\\n",
      "count                           240   \n",
      "unique                          240   \n",
      "top     AITA for making a dad joke?   \n",
      "freq                              1   \n",
      "\n",
      "                                                      url  \n",
      "count                                                 240  \n",
      "unique                                                240  \n",
      "top     https://www.reddit.com/r/AmItheAsshole/comment...  \n",
      "freq                                                    1  \n",
      "\n",
      "\n",
      " Summary of all of the posts in the \"No A-holes here\" archive:\n",
      "       archive      id                                           selftext  \\\n",
      "count      248     248                                                248   \n",
      "unique       1     248                                                248   \n",
      "top        NAH  e9ny7c  I'm getting a major surgery done in about thre...   \n",
      "freq       248       1                                                  1   \n",
      "\n",
      "                                                    title  \\\n",
      "count                                                 248   \n",
      "unique                                                248   \n",
      "top     AITA for accepting a drawing off my boyfriends...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                                      url  \n",
      "count                                                 248  \n",
      "unique                                                248  \n",
      "top     https://www.reddit.com/r/AmItheAsshole/comment...  \n",
      "freq                                                    1  \n"
     ]
    }
   ],
   "source": [
    "for judgement in archive_names.keys():\n",
    "    print(\"\\n\\n Summary of all of the posts in the \\\"\" + archive_names[judgement] + \"\\\" archive:\")\n",
    "    print(corpus.loc[corpus['archive'] == judgement].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the Corpus\n",
    "\n",
    "#### NLTK\n",
    "[Natural Language Toolkit (NLTK)](https://www.nltk.org/) is a python library with a lot of Natural Language processing tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
